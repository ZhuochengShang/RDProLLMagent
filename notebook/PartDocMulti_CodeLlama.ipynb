{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50fa6ff1-94f3-40c3-a961-037c4e7084eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, json, os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Iterable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851cb2e8-5295-43ae-b9aa-f2332cf7a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Paths (EDIT THESE)\n",
    "# ----------------------------\n",
    "DOCS_ROOT = Path(\"../doc/docs\")\n",
    "PY_PATH   = Path(\"../python/ndvi.py\")\n",
    "\n",
    "assert DOCS_ROOT.exists(), f\"Missing docs root: {DOCS_ROOT}\"\n",
    "assert PY_PATH.exists(), f\"Missing python script: {PY_PATH}\"\n",
    "\n",
    "py_code = PY_PATH.read_text(encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab3a1513-bb10-404c-bc34-48a62d38b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Markdown chunking utilities\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    heading_path: str\n",
    "    level: int\n",
    "    text: str\n",
    "\n",
    "def chunk_markdown_by_headings(md: str) -> List[Chunk]:\n",
    "    lines = md.splitlines()\n",
    "    starts = []\n",
    "    for i, line in enumerate(lines):\n",
    "        m = re.match(r\"^(#{2,3})\\s+(.*\\S)\\s*$\", line)\n",
    "        if m:\n",
    "            starts.append((i, len(m.group(1)), m.group(2).strip()))\n",
    "\n",
    "    if not starts:\n",
    "        return [Chunk(\"c0000\", \"DOC\", 1, md.strip() + \"\\n\")]\n",
    "\n",
    "    chunks: List[Chunk] = []\n",
    "    current_h2: Optional[str] = None\n",
    "\n",
    "    for idx, (start_i, level, title) in enumerate(starts):\n",
    "        end_i = starts[idx + 1][0] if idx + 1 < len(starts) else len(lines)\n",
    "        body = \"\\n\".join(lines[start_i:end_i]).strip() + \"\\n\"\n",
    "\n",
    "        if level == 2:\n",
    "            current_h2 = title\n",
    "            heading_path = current_h2\n",
    "        else:\n",
    "            heading_path = f\"{current_h2} / {title}\" if current_h2 else title\n",
    "\n",
    "        chunks.append(Chunk(f\"c{idx:04d}\", heading_path, level, body))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def make_doc_pack(selected: List[Chunk]) -> str:\n",
    "    out = []\n",
    "    for ch in selected:\n",
    "        out.append(f\"### DOC CHUNK {ch.chunk_id}: {ch.heading_path}\\n{ch.text}\\n\")\n",
    "    return \"\\n\".join(out).strip() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14fe8a4-df8e-47b5-af12-ea164a3761f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Docs loader: key -> .md file(s)\n",
    "# ----------------------------\n",
    "def load_docs_for_keys_with_aliases(\n",
    "    root: Path,\n",
    "    keys: Iterable[str],\n",
    "    *,\n",
    "    encoding: str = \"utf-8\",\n",
    "    aliases: Optional[Dict[str, str]] = None,\n",
    ") -> List[Tuple[Path, str]]:\n",
    "    \"\"\"\n",
    "    keys: things you care about (API names or concepts).\n",
    "    aliases: maps key -> filename_stem (e.g., 'mapPixels' -> 'mappixels').\n",
    "    Returns unique docs in first-seen order.\n",
    "    \"\"\"\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"DOCS_ROOT does not exist: {root}\")\n",
    "\n",
    "    aliases = aliases or {}\n",
    "\n",
    "    def norm(s: str) -> str:\n",
    "        return s.strip().lower()\n",
    "\n",
    "    # index: stem -> path\n",
    "    md_index: Dict[str, Path] = {}\n",
    "    for p in root.rglob(\"*.md\"):\n",
    "        md_index[norm(p.stem)] = p\n",
    "\n",
    "    resolved_paths: List[Path] = []\n",
    "    seen = set()\n",
    "\n",
    "    for k in keys:\n",
    "        stem = norm(aliases.get(k, k))  # apply alias if any\n",
    "\n",
    "        # direct stem match\n",
    "        path = md_index.get(stem)\n",
    "\n",
    "        # fallback: normalize to alnum only (handles camelCase-ish inputs)\n",
    "        if path is None:\n",
    "            stem2 = \"\".join(ch for ch in stem if ch.isalnum())\n",
    "            path = md_index.get(stem2)\n",
    "\n",
    "        if path is None:\n",
    "            available = sorted(md_index.keys())\n",
    "            raise FileNotFoundError(\n",
    "                f\"No .md doc for key '{k}' (resolved stem '{stem}') under {root}\\n\"\n",
    "                f\"Available docs: {available}\"\n",
    "            )\n",
    "\n",
    "        if path not in seen:\n",
    "            resolved_paths.append(path)\n",
    "            seen.add(path)\n",
    "\n",
    "    return [(p, p.read_text(encoding=encoding)) for p in resolved_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6468edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = os.environ.get(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "MODEL = os.environ.get(\"OLLAMA_MODEL\", \"codellama:latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ee3d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a geospatial data engineer and Spark systems expert.\n",
    "\n",
    "Task: Convert a given geospatial Python script into Scala code that runs on RDPro (Spark-based raster processing) on Apache Spark.\n",
    "\n",
    "You must understand Spark execution and produce distributed, RDD-based Scala.\n",
    "\n",
    "AUTHORITATIVE EXAMPLES RULE:\n",
    "- For each operation, Scala/Python examples may exist under examples/.\n",
    "- If an example file is missing or empty, ignore it.\n",
    "- Use only APIs/signatures shown in DOC CHUNKS OR non-empty examples.\n",
    "- Never assume Scala and Python examples are symmetric.\n",
    "\n",
    "Environment & paths:\n",
    "- Determine whether output paths should be treated as local or distributed based on Spark configuration and the URI scheme.\n",
    "- You MAY use standard Spark/Scala APIs for this (SparkConf, SparkContext.hadoopConfiguration, java.net.URI, java.nio.file).\n",
    "- You MUST NOT invent any RDPro path utilities.\n",
    "\n",
    "FILESYSTEM & PATH NORMALIZATION (MANDATORY):\n",
    "- Detect Spark local mode using SparkContext:\n",
    "  - Treat as local if `sc.master` starts with \"local\" (case-insensitive).\n",
    "- Before calling any RDPro IO API (e.g., geoTiff read/write), normalize ALL input/output paths:\n",
    "  1) If the path already has a URI scheme (file:, hdfs:, s3a:, gs:, http:, etc.), use it as-is.\n",
    "  2) If the path has NO scheme AND Spark is local AND the path looks like a local filesystem path\n",
    "     (e.g., starts with \"/\" on Unix/macOS, or has a Windows drive like \"C:\\\\\"), convert it to an\n",
    "     absolute `file:///...` URI using standard Java APIs (java.net.URI + java.nio.file.Paths).\n",
    "  3) If Spark is NOT local, do NOT prepend file:///; leave scheme-less paths unchanged so they\n",
    "     resolve against the cluster filesystem config (fs.defaultFS).\n",
    "- This rule exists to prevent Hadoop from interpreting local absolute paths as HDFS\n",
    "  (e.g., hdfs://localhost:9000).\n",
    "  \n",
    "Hard rules:\n",
    "1) Output MUST be valid Scala and compile as an RDPro operation module.\n",
    "   Required structure:\n",
    "   - `object <OperationName> { def run(sc: SparkContext): <ReturnType> = ... }`\n",
    "   - Include all necessary imports\n",
    "   - Do NOT define `main` and do NOT use `extends App`\n",
    "   - Do NOT create or stop SparkSession or SparkContext inside `run`\n",
    "   - Assume SparkContext `sc` is provided by the caller2) Use ONLY RDPro APIs that appear in the provided DOC CHUNKS.\n",
    "   - If a method signature is not shown in DOC CHUNKS, do NOT guess.\n",
    "3) Do NOT invent RDPro APIs, overloads, implicits, or helper utilities. No hidden \"magic\" conversions.\n",
    "4) Preserve semantics of the Python: raster IO, pixel math, focal ops, masking/nodata, reprojection/resample if present.\n",
    "5) Distributed correctness:\n",
    "   - Avoid driver-side operations: do NOT call collect/toLocalIterator unless required by the Python semantics.\n",
    "   - Prefer RDPro RasterRDD end-to-end when available in DOC CHUNKS.\n",
    "6) Raster alignment robustness:\n",
    "   - If not in DOC CHUNKS, fail fast: throw a clear runtime error explaining alignment is required but unsupported with available APIs.\n",
    "7) Performance guidance (Spark-level only):\n",
    "   - You MAY set Spark SQL / Spark configs and use standard Spark operations (repartition/coalesce/cache/persist) ONLY when:\n",
    "     (a) it does not change semantics, and\n",
    "     (b) it is justified by an obvious pipeline boundary (e.g., before a wide op / expensive reuse).\n",
    "8) Lambdas:\n",
    "   - When passing lambdas to RDPro functions (e.g., mapPixels), add explicit parameter and return types so Scala compiles.\n",
    "9) CLI args:\n",
    "   - If the Python has input/output paths, read them from args with safe defaults and validation.\n",
    "   - Do not introduce extra parameters not implied by the Python.\n",
    "\n",
    "Output format (strict):\n",
    "- First: Scala file content only (NO markdown fences).\n",
    "- After the Scala: a \"NOTES\" section listing:\n",
    "  (a) RDPro APIs used (names only)\n",
    "  (b) Unsupported operations and why (especially if missing alignment/warp APIs)\n",
    "  (c) Assumptions about IO paths / bands / nodata / CRS / environment detection logic\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42ce21d5-6f33-47f5-97ff-e185d7537fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_prompt(doc_pack: str, py_code: str) -> str:\n",
    "    return f\"\"\"\n",
    "RDPro documentation (relevant DOC CHUNKS only):\n",
    "{doc_pack}\n",
    "\n",
    "Python script:\n",
    "{py_code}\n",
    "\n",
    "Task:\n",
    "Translate the Python script into Scala targeting RDPro on Spark.\n",
    "Use ONLY APIs described in the DOC CHUNKS.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a49fd3f1-8ae0-4584-8024-9560fa1ad238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Manual \"keys\" (what you used before)\n",
    "# These are NOT necessarily filenames; aliases map them to doc files.\n",
    "# ----------------------------\n",
    "NDVI_KEYS = [\n",
    "    \"datamodel\",\n",
    "    \"setup\",\n",
    "    \"dataloading\",\n",
    "    \"rastermetadata\",\n",
    "    \"overlay\",\n",
    "    \"mapPixels\",\n",
    "    \"saveAsGeoTiff\",\n",
    "    \"GeoTiffWriter\",\n",
    "    \"Compression\",\n",
    "]\n",
    "\n",
    "ALIASES = {\n",
    "    # docs tree uses lowercase stems like mappixels.md\n",
    "    \"mapPixels\": \"mappixels\",\n",
    "\n",
    "    # concepts that live inside these docs (not separate files)\n",
    "    \"saveAsGeoTiff\": \"rasterwriting\",\n",
    "    \"GeoTiffWriter\": \"rasterwriting\",\n",
    "    \"Compression\": \"rasterwriting\",\n",
    "\n",
    "    # \"geoTiff\" read API is typically documented in dataloading\n",
    "    \"geoTiff\": \"dataloading\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25db4fb-49db-4c4c-9d46-95306e5c4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs:\n",
      " - ../doc/docs/common/datamodel.md\n",
      " - ../doc/docs/common/setup.md\n",
      " - ../doc/docs/data/dataloading.md\n",
      " - ../doc/docs/common/rastermetadata.md\n",
      " - ../doc/docs/process/overlay.md\n",
      " - ../doc/docs/process/mappixels.md\n",
      " - ../doc/docs/data/rasterwriting.md\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Load docs -> chunk -> pack -> final prompt\n",
    "# ----------------------------\n",
    "doc_files = load_docs_for_keys_with_aliases(DOCS_ROOT, NDVI_KEYS, aliases=ALIASES)\n",
    "\n",
    "print(\"Loaded docs:\")\n",
    "for p, _ in doc_files:\n",
    "    print(\" -\", p)\n",
    "\n",
    "all_chunks: List[Chunk] = []\n",
    "for path, text in doc_files:\n",
    "    chunks = chunk_markdown_by_headings(text)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "doc_pack = make_doc_pack(all_chunks)\n",
    "user_prompt = build_user_prompt(doc_pack, py_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d8e0ab-cef2-49e2-a8f7-acb66c9ed132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt chars: 12092\n",
      "Saved: runs/workspace/prompt_manual_multi_codellama.txt\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Save prompt + doc selection (optional)\n",
    "# ----------------------------\n",
    "OUT_DIR = Path(\"./runs/workspace\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(OUT_DIR / \"prompt_manual_multi_codellama.txt\").write_text(user_prompt, encoding=\"utf-8\")\n",
    "(OUT_DIR / \"doc_selection_multi_codellama.json\").write_text(\n",
    "    json.dumps([str(p) for p, _ in doc_files], indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"\\nPrompt chars:\", len(user_prompt))\n",
    "print(\"Saved:\", OUT_DIR / \"prompt_manual_multi_codellama.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac5bb63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ollama OK @ http://localhost:11434\n",
      "Available models: ['codellama:latest', 'llama3:latest', 'tinyllama:latest'] \n",
      "Selected MODEL: codellama:latest\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Ollama sanity check\n",
    "# ----------------------------\n",
    "def ollama_healthcheck() -> None:\n",
    "    import requests\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=10)\n",
    "        r.raise_for_status()\n",
    "        models = [m.get(\"name\") for m in r.json().get(\"models\", [])]\n",
    "        print(\"\\nOllama OK @\", OLLAMA_URL)\n",
    "        print(\"Available models:\", models[:10], \"...\" if len(models) > 10 else \"\")\n",
    "        print(\"Selected MODEL:\", MODEL)\n",
    "        if models and MODEL not in models:\n",
    "            print(\"WARNING: Selected MODEL not in /api/tags list. If calls fail, set OLLAMA_MODEL to one of the listed names.\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nOllama NOT reachable at\", OLLAMA_URL)\n",
    "        print(\"Error:\", repr(e))\n",
    "        raise\n",
    "\n",
    "ollama_healthcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc3d69f0-fded-4d09-85bd-642f9369fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote: runs/workspace/ndvi_doc_multi_codellama.scala\n",
      "LLM latency: 594.18 s\n",
      "\n",
      "--- Preview ---\n",
      " [PYTHON]\n",
      "import os\n",
      "from pathlib import Path\n",
      "\n",
      "import numpy as np\n",
      "from PIL import Image\n",
      "\n",
      "B4_PATH = \"/path/to/landsat8/LC08_L2SP_040037_20250827_20250903_02_T1_SR_B4.TIF\"  # Red\n",
      "B5_PATH = \"/path/to/landsat8/LC08_L2SP_040037_20250827_20250903_02_T1_SR_B5.TIF\"  # NIR\n",
      "OUT_NDVI = \"/path/to/ndvi.tif\"\n",
      "\n",
      "# Open datasets\n",
      "ds_red = gdal.Open(B4_PATH, gdal.GA_ReadOnly)\n",
      "ds_nir = gdal.Open(B5_PATH, gdal.GA_ReadOnly)\n",
      "\n",
      "assert ds_red and ds_nir, \"Failed to open input files\"\n",
      "\n",
      "# Read arrays\n",
      "red = np.array(Image.open(str(Path(B4_PATH).resolve().absolute())))\n",
      "nir = np.array(Image.open(str(Path(B5_PATH).resolve().absolute())))\n",
      "\n",
      "# Check grid alignment\n",
      "if (\n",
      "    ds_red.GetGeoTransform() != ds_nir.GetGeoTransform()\n",
      "    or ds_red.GetProjection() != ds_nir.GetProjection()\n",
      "):\n",
      "    raise RuntimeError(\"B4 and B5 grids do not match â€” warp one band first\")\n",
      "\n",
      "# Handle NoData\n",
      "red_nodata = ds_red.GetRasterBand(1).GetNoDataValue\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Run LLaMA locally via Ollama\n",
    "# ----------------------------\n",
    "def run_llm(prompt: str) -> Tuple[str, float]:\n",
    "    import requests\n",
    "    t0 = time.time()\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        # Optional tuning knobs (uncomment if desired):\n",
    "        # \"options\": {\"temperature\": 0.2, \"num_ctx\": 8192},\n",
    "    }\n",
    "    r = requests.post(f\"{OLLAMA_URL}/api/chat\", json=payload, timeout=600)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    text = data[\"message\"][\"content\"].strip()\n",
    "    return text, time.time() - t0\n",
    "\n",
    "scala_out, dt = run_llm(user_prompt)\n",
    "(OUT_DIR / \"ndvi_doc_multi_codellama.scala\").write_text(scala_out, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nWrote:\", OUT_DIR / \"ndvi_doc_multi_codellama.scala\")\n",
    "print(\"LLM latency:\", round(dt, 2), \"s\")\n",
    "print(\"\\n--- Preview ---\\n\", scala_out[:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc60da-ca53-4071-bd13-b3b82e53fee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
