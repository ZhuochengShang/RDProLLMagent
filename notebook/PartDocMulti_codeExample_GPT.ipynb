{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50fa6ff1-94f3-40c3-a961-037c4e7084eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, json, os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Iterable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "851cb2e8-5295-43ae-b9aa-f2332cf7a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Paths (EDIT THESE)\n",
    "# ----------------------------\n",
    "DOCS_ROOT     = Path(\"/Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs\")\n",
    "EXAMPLES_ROOT = Path(\"/Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs/examples\")\n",
    "PY_PATH       = Path(\"/Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/python/ndvi.py\")\n",
    "\n",
    "assert DOCS_ROOT.exists(), f\"Missing docs root: {DOCS_ROOT}\"\n",
    "assert EXAMPLES_ROOT.exists(), f\"Missing examples root: {EXAMPLES_ROOT}\"\n",
    "assert PY_PATH.exists(), f\"Missing python script: {PY_PATH}\"\n",
    "\n",
    "py_code = PY_PATH.read_text(encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab3a1513-bb10-404c-bc34-48a62d38b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Markdown chunking utilities\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    heading_path: str\n",
    "    level: int\n",
    "    text: str\n",
    "\n",
    "def chunk_markdown_by_headings(md: str) -> List[Chunk]:\n",
    "    lines = md.splitlines()\n",
    "    starts = []\n",
    "    for i, line in enumerate(lines):\n",
    "        m = re.match(r\"^(#{2,3})\\s+(.*\\S)\\s*$\", line)\n",
    "        if m:\n",
    "            starts.append((i, len(m.group(1)), m.group(2).strip()))\n",
    "\n",
    "    if not starts:\n",
    "        return [Chunk(\"c0000\", \"DOC\", 1, md.strip() + \"\\n\")]\n",
    "\n",
    "    chunks: List[Chunk] = []\n",
    "    current_h2: Optional[str] = None\n",
    "\n",
    "    for idx, (start_i, level, title) in enumerate(starts):\n",
    "        end_i = starts[idx + 1][0] if idx + 1 < len(starts) else len(lines)\n",
    "        body = \"\\n\".join(lines[start_i:end_i]).strip() + \"\\n\"\n",
    "\n",
    "        if level == 2:\n",
    "            current_h2 = title\n",
    "            heading_path = current_h2\n",
    "        else:\n",
    "            heading_path = f\"{current_h2} / {title}\" if current_h2 else title\n",
    "\n",
    "        chunks.append(Chunk(f\"c{idx:04d}\", heading_path, level, body))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def make_doc_pack(selected: List[Chunk]) -> str:\n",
    "    out = []\n",
    "    for ch in selected:\n",
    "        out.append(f\"### DOC CHUNK {ch.chunk_id}: {ch.heading_path}\\n{ch.text}\\n\")\n",
    "    return \"\\n\".join(out).strip() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e14fe8a4-df8e-47b5-af12-ea164a3761f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Docs loader: key -> .md file(s)\n",
    "# ----------------------------\n",
    "def load_docs_for_keys_with_aliases(\n",
    "    root: Path,\n",
    "    keys: Iterable[str],\n",
    "    *,\n",
    "    encoding: str = \"utf-8\",\n",
    "    aliases: Optional[Dict[str, str]] = None,\n",
    ") -> List[Tuple[Path, str]]:\n",
    "    \"\"\"\n",
    "    keys: things you care about (API names or concepts).\n",
    "    aliases: maps key -> filename_stem (e.g., 'mapPixels' -> 'mappixels').\n",
    "    Returns unique docs in first-seen order.\n",
    "    \"\"\"\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"DOCS_ROOT does not exist: {root}\")\n",
    "\n",
    "    aliases = aliases or {}\n",
    "\n",
    "    def norm(s: str) -> str:\n",
    "        return s.strip().lower()\n",
    "\n",
    "    # index: stem -> path\n",
    "    md_index: Dict[str, Path] = {}\n",
    "    for p in root.rglob(\"*.md\"):\n",
    "        md_index[norm(p.stem)] = p\n",
    "\n",
    "    resolved_paths: List[Path] = []\n",
    "    seen = set()\n",
    "\n",
    "    for k in keys:\n",
    "        stem = norm(aliases.get(k, k))  # apply alias if any\n",
    "\n",
    "        # direct stem match\n",
    "        path = md_index.get(stem)\n",
    "\n",
    "        # fallback: normalize to alnum only (handles camelCase-ish inputs)\n",
    "        if path is None:\n",
    "            stem2 = \"\".join(ch for ch in stem if ch.isalnum())\n",
    "            path = md_index.get(stem2)\n",
    "\n",
    "        if path is None:\n",
    "            available = sorted(md_index.keys())\n",
    "            raise FileNotFoundError(\n",
    "                f\"No .md doc for key '{k}' (resolved stem '{stem}') under {root}\\n\"\n",
    "                f\"Available docs: {available}\"\n",
    "            )\n",
    "\n",
    "        if path not in seen:\n",
    "            resolved_paths.append(path)\n",
    "            seen.add(path)\n",
    "\n",
    "    return [(p, p.read_text(encoding=encoding)) for p in resolved_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3395b1e7-af97-4b38-aab4-b6e2af089a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Examples loader: operation -> (scala?, python?)\n",
    "# ----------------------------\n",
    "def load_examples_for_operations(\n",
    "    examples_root: Path,\n",
    "    operations: Iterable[str],\n",
    "    *,\n",
    "    encoding: str = \"utf-8\",\n",
    ") -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    For each operation (e.g. '00_dataloading'):\n",
    "      - load examples/<op>.scala if it exists and is non-empty\n",
    "      - load examples/<op>.py if it exists and is non-empty\n",
    "\n",
    "    Missing/empty files are skipped.\n",
    "    \"\"\"\n",
    "    if not examples_root.exists():\n",
    "        raise FileNotFoundError(f\"EXAMPLES_ROOT does not exist: {examples_root}\")\n",
    "\n",
    "    results: Dict[str, Dict[str, str]] = {}\n",
    "\n",
    "    for op in operations:\n",
    "        op_key = op.strip()\n",
    "        bucket: Dict[str, str] = {}\n",
    "\n",
    "        scala_path = examples_root / f\"{op_key}.scala\"\n",
    "        if scala_path.exists():\n",
    "            text = scala_path.read_text(encoding=encoding).strip()\n",
    "            if text:\n",
    "                bucket[\"scala\"] = text\n",
    "\n",
    "        py_path = examples_root / f\"{op_key}.py\"\n",
    "        if py_path.exists():\n",
    "            text = py_path.read_text(encoding=encoding).strip()\n",
    "            if text:\n",
    "                bucket[\"python\"] = text\n",
    "\n",
    "        if bucket:\n",
    "            results[op_key] = bucket\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aba3245b-acda-4979-8b9b-f4ffb1567865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_examples_pack(examples: Dict[str, Dict[str, str]]) -> str:\n",
    "    if not examples:\n",
    "        return \"\"\n",
    "    parts: List[str] = []\n",
    "    for op, langs in examples.items():\n",
    "        parts.append(f\"## EXAMPLE OPERATION: {op}\")\n",
    "        if \"scala\" in langs:\n",
    "            parts.append(\"### Scala example\\n\" + langs[\"scala\"].strip() + \"\\n\")\n",
    "        if \"python\" in langs:\n",
    "            parts.append(\"### Python example\\n\" + langs[\"python\"].strip() + \"\\n\")\n",
    "    return \"\\n\".join(parts).strip() + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad98cf0b-627c-412c-96b8-08cc6583ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-5\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a geospatial data engineer and Spark systems expert.\n",
    "\n",
    "Task: Convert a given geospatial Python script into Scala code that runs on RDPro (Spark-based raster processing) on Apache Spark.\n",
    "\n",
    "You must understand Spark execution and produce distributed, RDD-based Scala.\n",
    "\n",
    "AUTHORITATIVE EXAMPLES RULE:\n",
    "- For each operation, Scala/Python examples may exist under examples/.\n",
    "- If an example file is missing or empty, ignore it.\n",
    "- Use only APIs/signatures shown in DOC CHUNKS OR non-empty examples.\n",
    "- Never assume Scala and Python examples are symmetric.\n",
    "\n",
    "Environment & paths:\n",
    "- Determine whether output paths should be treated as local or distributed based on Spark configuration and the URI scheme.\n",
    "- You MAY use standard Spark/Scala APIs for this (SparkConf, SparkContext.hadoopConfiguration, java.net.URI, java.nio.file).\n",
    "- You MUST NOT invent any RDPro path utilities.\n",
    "\n",
    "FILESYSTEM & PATH NORMALIZATION (MANDATORY):\n",
    "- Detect Spark local mode using SparkContext:\n",
    "  - Treat as local if `sc.master` starts with \"local\" (case-insensitive).\n",
    "- Before calling any RDPro IO API (e.g., geoTiff read/write), normalize ALL input/output paths:\n",
    "  1) If the path already has a URI scheme (file:, hdfs:, s3a:, gs:, http:, etc.), use it as-is.\n",
    "  2) If the path has NO scheme AND Spark is local AND the path looks like a local filesystem path\n",
    "     (e.g., starts with \"/\" on Unix/macOS, or has a Windows drive like \"C:\\\\\"), convert it to an\n",
    "     absolute `file:///...` URI using standard Java APIs (java.net.URI + java.nio.file.Paths).\n",
    "  3) If Spark is NOT local, do NOT prepend file:///; leave scheme-less paths unchanged so they\n",
    "     resolve against the cluster filesystem config (fs.defaultFS).\n",
    "- This rule exists to prevent Hadoop from interpreting local absolute paths as HDFS\n",
    "  (e.g., hdfs://localhost:9000).\n",
    "\n",
    "Hard rules:\n",
    "1) Output MUST be valid Scala and compile as an RDPro operation module.\n",
    "   Required structure:\n",
    "   - `object <OperationName> { def run(sc: SparkContext): <ReturnType> = ... }`\n",
    "   - Include all necessary imports\n",
    "   - Do NOT define `main` and do NOT use `extends App`\n",
    "   - Do NOT create or stop SparkSession or SparkContext inside `run`\n",
    "   - Assume SparkContext `sc` is provided by the caller\n",
    "2) Use ONLY RDPro APIs that appear in the provided DOC CHUNKS OR non-empty examples.\n",
    "   - If a method signature is not shown, do NOT guess.\n",
    "3) Do NOT invent RDPro APIs, overloads, implicits, or helper utilities.\n",
    "4) Preserve semantics of the Python.\n",
    "5) Distributed correctness: avoid driver-side collect unless required.\n",
    "6) Raster alignment: if required but no API exists in docs/examples, throw a clear runtime error.\n",
    "7) Performance guidance: only safe Spark-level optimizations.\n",
    "8) Lambdas: add explicit parameter/return types.\n",
    "9) CLI args:\n",
    "   - If the Python has input/output paths, read them from args with safe defaults and validation.\n",
    "   - Do not introduce extra parameters not implied by the Python.\n",
    "\n",
    "Output format (strict):\n",
    "- First: Scala file content only (NO markdown fences).\n",
    "- After the Scala: a \"NOTES\" section listing:\n",
    "  (a) RDPro APIs used (names only)\n",
    "  (b) Unsupported operations and why\n",
    "  (c) Assumptions about IO paths / bands / nodata / CRS / environment detection logic\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42ce21d5-6f33-47f5-97ff-e185d7537fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_prompt(doc_pack: str, examples_pack: str, py_code: str) -> str:\n",
    "    return f\"\"\"\n",
    "RDPro documentation (relevant DOC CHUNKS only):\n",
    "{doc_pack}\n",
    "\n",
    "Non-empty code examples (authoritative; missing/empty files are omitted):\n",
    "{examples_pack}\n",
    "\n",
    "Python script:\n",
    "{py_code}\n",
    "\n",
    "Task:\n",
    "Translate the Python script into Scala targeting RDPro on Spark.\n",
    "Use ONLY APIs/signatures described in the DOC CHUNKS or shown in examples.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a49fd3f1-8ae0-4584-8024-9560fa1ad238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Manual \"keys\" (what you used before)\n",
    "# These are NOT necessarily filenames; aliases map them to doc files.\n",
    "# ----------------------------\n",
    "NDVI_KEYS = [\n",
    "    \"datamodel\",\n",
    "    \"setup\",\n",
    "    \"dataloading\",\n",
    "    \"rastermetadata\",\n",
    "    \"overlay\",\n",
    "    \"mapPixels\",\n",
    "    \"saveAsGeoTiff\",\n",
    "    \"GeoTiffWriter\",\n",
    "    \"Compression\",\n",
    "]\n",
    "\n",
    "ALIASES = {\n",
    "    # docs tree uses lowercase stems like mappixels.md\n",
    "    \"mapPixels\": \"mappixels\",\n",
    "\n",
    "    # concepts that live inside these docs (not separate files)\n",
    "    \"saveAsGeoTiff\": \"rasterwriting\",\n",
    "    \"GeoTiffWriter\": \"rasterwriting\",\n",
    "    \"Compression\": \"rasterwriting\",\n",
    "\n",
    "    # \"geoTiff\" read API is typically documented in dataloading\n",
    "    \"geoTiff\": \"dataloading\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96b097d5-07f9-493f-b220-ade18ef6325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_OPERATIONS = [\n",
    "    \"00_dataloading\",\n",
    "    \"02_mappixels\",\n",
    "    \"03_overlay\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b25db4fb-49db-4c4c-9d46-95306e5c4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs:\n",
      " - /Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs/common/datamodel.md\n",
      " - /Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs/common/setup.md\n",
      " - /Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs/data/dataloading.md\n",
      " - /Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs/common/rastermetadata.md\n",
      " - /Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs/process/overlay.md\n",
      " - /Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs/process/mappixels.md\n",
      " - /Users/clockorangezoe/Documents/phd_projects/code/geoAI/RDProLLMagent/doc/docs/data/rasterwriting.md\n",
      "\n",
      "Loaded examples (non-empty only):\n",
      " - 00_dataloading -> python,scala\n",
      " - 02_mappixels -> python,scala\n",
      " - 03_overlay -> python,scala\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Load docs -> chunk -> pack\n",
    "# ----------------------------\n",
    "doc_files = load_docs_for_keys_with_aliases(DOCS_ROOT, NDVI_KEYS, aliases=ALIASES)\n",
    "\n",
    "print(\"Loaded docs:\")\n",
    "for p, _ in doc_files:\n",
    "    print(\" -\", p)\n",
    "\n",
    "all_chunks: List[Chunk] = []\n",
    "for path, text in doc_files:\n",
    "    all_chunks.extend(chunk_markdown_by_headings(text))\n",
    "\n",
    "doc_pack = make_doc_pack(all_chunks)\n",
    "\n",
    "# ----------------------------\n",
    "# Load examples -> pack (skips empty/missing)\n",
    "# ----------------------------\n",
    "examples = load_examples_for_operations(EXAMPLES_ROOT, EXAMPLE_OPERATIONS)\n",
    "examples_pack = make_examples_pack(examples)\n",
    "\n",
    "print(\"\\nLoaded examples (non-empty only):\")\n",
    "for op, langs in examples.items():\n",
    "    print(\" -\", op, \"->\", \",\".join(sorted(langs.keys())))\n",
    "\n",
    "# ----------------------------\n",
    "# Build final prompt\n",
    "# ----------------------------\n",
    "user_prompt = build_user_prompt(doc_pack, examples_pack, py_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68d8e0ab-cef2-49e2-a8f7-acb66c9ed132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt chars: 19517\n",
      "Saved: runs/workspace/prompt_manual_multi.txt\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Save prompt + selections\n",
    "# ----------------------------\n",
    "OUT_DIR = Path(\"./runs/workspace\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(OUT_DIR / \"prompt_manual_multi.txt\").write_text(user_prompt, encoding=\"utf-8\")\n",
    "(OUT_DIR / \"doc_selection_multi.json\").write_text(\n",
    "    json.dumps([str(p) for p, _ in doc_files], indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "(OUT_DIR / \"example_selection_multi.json\").write_text(\n",
    "    json.dumps({op: list(langs.keys()) for op, langs in examples.items()}, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"\\nPrompt chars:\", len(user_prompt))\n",
    "print(\"Saved:\", OUT_DIR / \"prompt_manual_multi.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2de4573b-87e7-4e05-8a1f-3d0ef6913852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" in os.environ:\n",
    "    print(\"OPENAI_API_KEY is set\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is NOT set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc3d69f0-fded-4d09-85bd-642f9369fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: runs/workspace/ndvi_doc_multi_codeExample_GPT.scala\n",
      "LLM latency: 116.94 s\n",
      "\n",
      "--- Preview ---\n",
      " import edu.ucr.cs.bdlab.beast._\n",
      "import edu.ucr.cs.bdlab.raptor.RasterOperationsLocal\n",
      "import org.apache.spark.SparkContext\n",
      "\n",
      "import java.net.URI\n",
      "import java.nio.file.{Paths, Path => JPath}\n",
      "\n",
      "object ndvi {\n",
      "\n",
      "  private def hasScheme(p: String): Boolean = {\n",
      "    try {\n",
      "      val u = new URI(p)\n",
      "      u.getScheme != null\n",
      "    } catch {\n",
      "      case _: Exception => false\n",
      "    }\n",
      "  }\n",
      "\n",
      "  // FILESYSTEM & PATH NORMALIZATION (MANDATORY)\n",
      "  private def normalizePath(rawPath: String, sc: SparkContext): String = {\n",
      "    if (rawPath == null || rawPath.trim.isEmpty) {\n",
      "      throw new IllegalArgumentException(\"Path must be non-empty\")\n",
      "    }\n",
      "    val isLocal: Boolean = sc.master != null && sc.master.toLowerCase.startsWith(\"local\")\n",
      "    if (hasScheme(rawPath)) {\n",
      "      rawPath\n",
      "    } else {\n",
      "      // No scheme\n",
      "      val jpath: JPath = Paths.get(rawPath)\n",
      "      val looksLocalAbsolute: Boolean = {\n",
      "        try {\n",
      "          jpath.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def run_llm(prompt: str) -> Tuple[str, float]:\n",
    "    t0 = time.time()\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return resp.output_text.strip(), time.time() - t0\n",
    "\n",
    "scala_out, dt = run_llm(user_prompt)\n",
    "(OUT_DIR / \"Job.manual.codeExample.scala\").write_text(scala_out, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\", OUT_DIR / \"ndvi_doc_multi_codeExample_GPT.scala\")\n",
    "print(\"LLM latency:\", round(dt, 2), \"s\")\n",
    "print(\"\\n--- Preview ---\\n\", scala_out[:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc60da-ca53-4071-bd13-b3b82e53fee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
